----
title: "Strategy Simulation Iternum 2"
output: html_document
---

```{r libraries, echo=FALSE}

library(lme4) # need for running lmer
library(tidyverse)
library(coda)
require(languageR)
library(lmerTest) #need this for lmer p values!!! Maybe it's not so important...
library(emmeans)
library(data.table)
library(ggplot2)
library(sjPlot)
library(stats4)
library(psych) # used for logit and logistic transformations
require(gridExtra) # for arranging multi-panel plots


```


What do we want to do?
Two kinds of strategies: compressed numerals, and 1-to-1 numerals. Our data has 2016 observations, some proportion of which were drawn from each of these two strategies (e.g. 40% compressed and 60% 1-to-1).

Therefore, we want to build two models, one for each strategy, and then reproduce our dataset using some balance between those two strategies.


```{r visualize_data, echo=FALSE}


df2 <- read_csv("../results/csv/iternum_data2_incl.csv")

df2reg <- df2 %>% filter(Regularity == 'regular')
df2ran <- df2 %>% filter(Regularity == 'random')

# hist(df2$strokeRatio,breaks=100)
# 
# ggplot(df2, aes(x = strokeRatio)) + 
#   geom_histogram(aes(y = ..count.., fill = Regularity), binwidth = .01, color = 'black', 
#                  alpha = .3,position="identity") 

```

```{r write_functions, echo=FALSE}


# write the log-likelihood function
loglik = function(DATA, match_percent, std1,   mean_comp, std2) { # compr_percent not a free param
  data = na.omit(DATA) # df2$strokeRatio[294] is NA, for example
  
  # first generate the probability of seeing every datapoint from the matching distribution
  matching_lik = dnorm(data, mean = 1, sd = std1) #* std1  # take sum of this, or later?
  # then generate that probability from the compressed distribution
  compress_lik = dnorm(data, mean = mean_comp, sd = std2) #* std2
  # now, for every datapoint, say which distribution more likely produced it
  argmaxes = as.numeric(apply(cbind(matching_lik,compress_lik), 1, which.max)) - 1 # get zero-indexed so that we can just add up the ones
  # figuring out argmax took longer than it was supposed to. Why can't I call "which.pmax"??
  # https://stackoverflow.com/questions/17252798/using-argmax-or-something-simpler-in-r
  # https://rdrr.io/cran/omnibus/src/R/which.pmax.r

  # now get an estimate of how many trials best fit the 'compressing' strategy (index=1)
  q = sum(argmaxes) / length(argmaxes)
  
  ll = sum(log(pmax(matching_lik,compress_lik)))
  
    # # this block used to exist when distinguishing perfect-matchers vs approximators. It never worked anyway
    # ifelse(data == 1, # if the data point is exactly 1, it might just be perfect
    #        sample(c(log(max(matching_lik,compress_lik)),0),1,prob=c(1-match_percent,match_percent)),
    #        log(pmax(matching_lik,compress_lik))
    #        )
  
  return(c(ll , q)) # return both the log likelihood of the whole model, and estimate of strategies
}

# Use this to generate fake data
get_estimate = function(match_percent, sd1,   mean_comp, sd2, compr_percent) {
  # sample from normal distribution for each: return 0 if sample < 0, 2 if sample > 2
  
  # matching_estimate = sample(c(1 , min(max(rnorm(1, 1, sd1), 0), 2)), # only use if distinguishing approx vs exact
  #                            size=1,
  #                            prob=c(match_percent,1-match_percent))
  
  matching_estimate = min(max(rnorm(1, 1, sd1), 0), 2)
  compress_estimate = min(max(rnorm(1, mean_comp, sd2), 0), 2)
  
  final_estimate = sample(c(matching_estimate,compress_estimate),
                          size = 1,
                          prob = c(1-compr_percent,compr_percent))
  return (final_estimate)
}

plot_estimates = function(D, winning_params){
  SAMPLE_N = length(D$strokeRatio)
  cur_estimates = c()
  for (i in 1:SAMPLE_N){
      cur_estimates = append(cur_estimates,get_estimate(winning_params$p,
                                                        winning_params$sd_approx,
                                                        winning_params$avg_comp,
                                                        winning_params$sd_comp,
                                                        winning_params$q
                                                        ))
  }
  
  
  ggplot() +
    geom_histogram(data = data.frame(cur_estimates), aes(x = cur_estimates, y = ..count..),
                   binwidth = .01, color = 'red', fill = 'red',
                   alpha = .3,position="identity") + xlim(0,max(df2$strokeRatio,na.rm=TRUE)) + # keep x-limit universal
    geom_histogram(data=D, aes(x = strokeRatio, y = ..count..),
                   binwidth = .01, color = 'black', fill = 'black',
                   alpha = .3,position="identity") + xlim(0,max(df2$strokeRatio,na.rm=TRUE))   # keep x-limit universal
}

# this is the function that will search over parameter space for best fitting model, given a likelihood function
maximize_LL = function(D){
  
  SAMPLE_N = length(D$strokeRatio) # number of sample "participants" to use in simulation data, 2016 for all data
  
  loglik_estimates = data.frame(loglik = c(),
                                q = c(),
                                p = c(),
                                sd_approx = c(),
                                avg_comp = c(),
                                sd_comp = c())
  
  #create data frame with 0 rows and 6 columns
  loglik_estimates <- data.frame(matrix(ncol = 6, nrow = 0))
  
  #provide column names
  colnames(loglik_estimates) <- c('loglik', 'q', 'p', 'sd_approx', 'avg_comp', 'sd_comp')
  
  
  prop_seq = seq(0, 0, .1) # this is trivial now and we're not using it
  sd1_seq  = seq(0, .2, .02)
  avg_seq  = seq(0, .6, .02)
  sd2_seq  = seq(0, .3, .02)
  
  steps = length(prop_seq) * length(sd1_seq) * length(avg_seq) * length(sd2_seq)
  x <- rep(x = NA, times = steps)
  pb <- txtProgressBar(0, length(x), style = 3)
  prog = 1;
  
  # Now brute-force it
  for (prop in prop_seq){            # for every % of perfect matchers
    for (sd1 in sd1_seq){            # for every sd of approximators
      for (avg in avg_seq){          # for every mean of compressed
        for (sd2 in sd2_seq){        # for every sd of compressed
          
          setTxtProgressBar(pb, prog) # update the progress bar
          x[prog] 
          prog = prog + 1
          
          simulation = loglik(D$strokeRatio,
                              prop,sd1, # % of perfect matchers, sd of approximators
                              avg,sd2) # mean of compressed, sd of compressed
          loglik_estimates = loglik_estimates %>% add_row(loglik = simulation[1],
                                       q = simulation[2],
                                       p = prop,
                                       sd_approx = sd1,
                                       avg_comp = avg,
                                       sd_comp = sd2)
          
        }
      }
    }
  }
  close(pb)
  
  # sometimes we're estimating infinite log likelihood. Is this a problem? Get rid of them, for now
  loglik_estimates <- loglik_estimates[is.finite(rowSums(loglik_estimates)),]
  
  # find the maximum log likelihood and get the parameters from that
  winning_params = loglik_estimates[which.max(loglik_estimates$loglik),]
  
  # this isn't a problem now, but might want it while debugging if/when re-incorporating exact vs approximate matchers:
  # print("We're not producing many unique log likelihood estimates: ")
  # print(length(unique(loglik_estimates$loglik)))
  
  return(c(winning_params , loglik_estimates))
}



```

``` {r plot_hists}
# now we can try this separately for each of the experimental conditions and see if it's different!
all <- maximize_LL(df2) # returns both winning param estimates and everything that was tried
ran <- maximize_LL(df2ran)
reg <- maximize_LL(df2reg)

all_params <- data.frame(all[1:6]) # just winning param estimates
ran_params <- data.frame(ran[1:6])
reg_params <- data.frame(reg[1:6])

all_all_params <- data.frame(all[7:12]) # all param estimates
ran_all_params <- data.frame(ran[7:12])
reg_all_params <- data.frame(reg[7:12])


plot_estimates(df2, all_params)
plot_estimates(df2ran, ran_params)
plot_estimates(df2reg, reg_params)

```

``` {r estimate_uncertainty}

# we didn't include uncertainty around parameter estimates above, so let's do that now
draws = 3 # 500

# step 1: sample participants randomly with replacement from each condition, many times
# we're going to do this a bunch of times, and generate winning parameters each time via MLE
# the these winning estimates over each sampled set of participants will form a distribution
# and that distribution will be our uncertainty (hopefully the distribution of each condition is either obviously null, or is very obviously different to the other condition)


#create data frame with 0 rows and 5 columns (p is obsolete)
ran_ps <- data.frame(matrix(ncol = 6, nrow = 0))
reg_ps <- data.frame(matrix(ncol = 6, nrow = 0))
ex3_ps <- data.frame(matrix(ncol = 6, nrow = 0))

#provide column names
colnames(ran_ps) <- c('cond','loglik', 'q', 'sd_approx', 'avg_comp', 'sd_comp')
colnames(reg_ps) <- c('cond','loglik', 'q', 'sd_approx', 'avg_comp', 'sd_comp')
colnames(ex3_ps) <- c('cond','loglik', 'q', 'sd_approx', 'avg_comp', 'sd_comp')


for (i in 1:draws){
  print(i)
  
  # first generate the list of games, then sample that list, then sample the dataframe
  # then maximize_LL on the sample:
  ran <- maximize_LL(df2ran[df2ran$gameID %in% sample(unique(df2ran$gameID), 29, replace = TRUE), ])[1:6]
  # we only add winning estimates to our list; one for each sample of games
  ran_ps = ran_ps %>% add_row(cond=0,
                                      loglik=ran$loglik,
                                      q=ran$q,
                                      sd_approx=ran$sd_approx,
                                      avg_comp=ran$avg_comp,
                                      sd_comp=ran$sd_comp) 
  
  reg <- maximize_LL(df2reg[df2reg$gameID %in% sample(unique(df2reg$gameID), 30, replace = TRUE), ])[1:6]
  
  
  reg_ps = reg_ps %>% add_row(cond=1,
                                      loglik=reg$loglik,
                                      q=reg$q,
                                      sd_approx=reg$sd_approx,
                                      avg_comp=reg$avg_comp,
                                      sd_comp=reg$sd_comp)
  
  # parameters here: data, number of samples (define it as number of games, with replacement)
  ex3 <- maximize_LL(df3[df3$gameID %in% sample(unique(df3$gameID), length(unique(df3$gameID)), replace = TRUE), ])[1:6]
  ex3_ps <- ex3_ps %>% add_row(cond=2,
                                      loglik=ex3$loglik,
                                      q=ex3$q,
                                      sd_approx=ex3$sd_approx,
                                      avg_comp=ex3$avg_comp,
                                      sd_comp=ex3$sd_comp)
  
}

ps = rbind(ran_ps,reg_ps, ex3_ps)
ps$cond <- gsub("0", "Random", ps$cond)
ps$cond <- gsub("1", "Regular", ps$cond)
ps$cond <- gsub("2", "Third", ps$cond)

```

``` {r plot_uncertainty}

## WAIT!!! Don't just do a normal 95-CI: you actually just need to COUNT, how many of the simulations were more extreme. So just take the game that is closest to the 97.5%th percentile, and that's your top 95-CI, ditto for the bottom one
print( c("q",quantile(ran_ps$q, probs=c(.025,.975)),
       quantile(reg_ps$q, probs=c(.025,.975))))

print( c("sd_approx",quantile(ran_ps$sd_approx, probs=c(.025,.975)),
       quantile(reg_ps$sd_approx, probs=c(.025,.975))))

print( c("avg_comp",quantile(ran_ps$avg_comp, probs=c(.025,.975)),
       quantile(reg_ps$avg_comp, probs=c(.025,.975))))

print( c("sd_comp",quantile(ran_ps$sd_comp, probs=c(.025,.975)),
       quantile(reg_ps$sd_comp, probs=c(.025,.975))))

# we have to get the mean and percentiles separately because apparently R is unable to do concurrent
qmean <- ps %>% group_by(cond) %>% group_map(~ mean(.x$q)) %>% as.numeric()
qCI <- ps %>% group_by(cond) %>% group_map(~ quantile(.x$q, probs = c(0.025, 0.975)))
avg_compmean <- ps %>% group_by(cond) %>% group_map(~ mean(.x$avg_comp)) %>% as.numeric()
avg_compCI <- ps %>% group_by(cond) %>% group_map(~ quantile(.x$avg_comp, probs = c(0.025, 0.975)))
sd_approxmean <- ps %>% group_by(cond) %>% group_map(~ mean(.x$sd_approx)) %>% as.numeric()
sd_approxCI <- ps %>% group_by(cond) %>% group_map(~ quantile(.x$sd_approx, probs = c(0.025, 0.975)))
sd_compmean <- ps %>% group_by(cond) %>% group_map(~ mean(.x$sd_comp)) %>% as.numeric()
sd_compCI <- ps %>% group_by(cond) %>% group_map(~ quantile(.x$sd_comp, probs = c(0.025, 0.975)))
  
# we also have to write out the code 4 separate times because we can't just for-loop it
plot1 <- ggplot() +
  geom_bar( aes(x=c("Random","Regular"),
                  y=qmean), stat="identity", fill="skyblue", alpha=0.7)+
  geom_errorbar( aes(x=c("Random","Regular"),
                       ymin=c(qCI[[1]][['2.5%']],qCI[[2]][['2.5%']]),
                       ymax=c(qCI[[1]][['97.5%']],qCI[[2]][['97.5%']])),
                 width=0.4, colour="black", alpha=0.9, size=.5)

plot2 <- ggplot() +
  geom_bar( aes(x=c("Random","Regular"),
                  y=avg_compmean), stat="identity", fill="skyblue", alpha=0.7)+
  geom_errorbar( aes(x=c("Random","Regular"),
                       ymin=c(avg_compCI[[1]][['2.5%']],avg_compCI[[2]][['2.5%']]),
                       ymax=c(avg_compCI[[1]][['97.5%']],avg_compCI[[2]][['97.5%']])),
                 width=0.4, colour="black", alpha=0.9, size=.5)

plot3 <- ggplot() +
  geom_bar( aes(x=c("Random","Regular"),
                  y=sd_approxmean), stat="identity", fill="skyblue", alpha=0.7)+
  geom_errorbar( aes(x=c("Random","Regular"),
                       ymin=c(sd_approxCI[[1]][['2.5%']],sd_approxCI[[2]][['2.5%']]),
                       ymax=c(sd_approxCI[[1]][['97.5%']],sd_approxCI[[2]][['97.5%']])),
                 width=0.4, colour="black", alpha=0.9, size=.5)

plot4 <- ggplot() +
  geom_bar( aes(x=c("Random","Regular"),
                  y=sd_compmean), stat="identity", fill="skyblue", alpha=0.7)+
  geom_errorbar( aes(x=c("Random","Regular"),
                       ymin=c(sd_compCI[[1]][['2.5%']],sd_compCI[[2]][['2.5%']]),
                       ymax=c(sd_compCI[[1]][['97.5%']],sd_compCI[[2]][['97.5%']])),
                 width=0.4, colour="black", alpha=0.9, size=.5)


grid.arrange(plot1, plot2, plot3, plot4, ncol=4)

# might be nice to plot it better, like here:
# https://felixfan.github.io/bar-plot/

# we can also plot a scatterplot of each param, like this:
ggplot() +
    geom_jitter(aes(x = factor(cond), y = avg_comp),
                data = ps,
                width = 0.1) +
    xlab("Condition") +
    ylab("avg_comp") +
    theme_bw()

ggplot(ps, aes(factor(cond), avg_comp)) + geom_violin() + geom_jitter(height = 0, width = 0.1)


```

``` {r WRONG_uncertainty_measure}


cond_means <- params %>%
  group_by(cond) %>%
  summarise_at(vars(param), list(param = mean))

cond_sds <- params %>%
  group_by(cond) %>%
  summarise_at(vars(param), list(param = sd))


p <- mutate_all(params, function(x) as.numeric(as.character(x)))


p0 <- p %>% filter(cond == 0)
p1 <- p %>% filter(cond == 1)
t = t.test(p0$avg_comp,p1$avg_comp)
t0 = t.test(p0$avg_comp)
t1 = t.test(p1$avg_comp)


# doesn't seem to work, even though we set everything as numeric
# https://stackoverflow.com/questions/35953394/calculating-length-of-95-ci-using-dplyr
cond_CIs <- p %>%
  group_by(cond) %>%
  summarise(mean.p = mean(param, na.rm = TRUE),
            sd.p = sd(param, na.rm = TRUE),
            n.p = n()) 
# %>%
#   mutate(se.param = sd.param / sqrt(n.param),
#          lower.ci.param = mean.param - qt(1 - (0.05 / 2), n.param - 1) * se.param,
#          upper.ci.param = mean.param + qt(1 - (0.05 / 2), n.param - 1) * se.param)



barplot(height=c(t0$statistic),yerr=c(t0$conf.int))


# https://r-graph-gallery.com/4-barplot-with-error-bar.html
# https://felixfan.github.io/bar-plot/

# Calculates mean, sd, se and IC
qCIs <- params %>%
  group_by(cond) %>%
  summarise( 
    n=n(),
    mean=mean(q),
    sd=sd(q)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1)) %>% mutate(param='q')

sd_approxCIs <- params %>%
  group_by(cond) %>%
  summarise( 
    n=n(),
    mean=mean(sd_approx),
    sd=sd(sd_approx)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1)) %>% mutate(param='sd_approx')

avg_compCIs <- params %>%
  group_by(cond) %>%
  summarise( 
    n=n(),
    mean=mean(avg_comp),
    sd=sd(avg_comp)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1)) %>% mutate(param='avg_comp')

sd_compCIs <- params %>%
  group_by(cond) %>%
  summarise( 
    n=n(),
    mean=mean(sd_comp),
    sd=sd(sd_comp)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1)) %>% mutate(param='sd_comp')


CIs = rbind(qCIs,sd_approxCIs,avg_compCIs,sd_compCIs)

# Confidence Interval
ggplot(CIs) +
  geom_bar( aes(x=cond, y=mean), stat="identity", fill="forestgreen", alpha=0.5) +
  geom_errorbar( aes(x=cond, ymin=mean-ic, ymax=mean+ic), width=0.4, colour="orange", alpha=0.9, size=.5) +
  facet_wrap(~ param)






```

``` {r blah}

# plot(sort(unique(loglik_estimates$loglik)))
# 
# hist(loglik_estimates$loglik)


# set.seed(5)

# sample two distributions, with the same number of datapoints from each
x1 = rnorm(10000, mean=1, sd=.05) # a vector of narrow distribution datapoints
x2 = rnorm(10000, mean=1, sd=.5 ) # a vector of wide distribution datapoints

# compare LLs
c( sum(log(dnorm(x1, mean = 1, sd = .05 )) /.05 ) , sum(log(dnorm(x2, mean = 1, sd = .5  )) / .05 ) )




testing =  all_all_params[all_all_params['avg_comp'] == 0.28]
testing =  all_all_params[all_all_params['loglik'] > -2000]
testing =  all_all_params %>% filter(loglik > -2000)




testing = testing[order(testing$avg_comp),] %>%
    mutate(rank = row_number())
ggplot(testing, aes(x=sd_comp, y=loglik)) +
  geom_point(size=.1)


ran_all_params = ran_all_params %>%
    mutate(rank = row_number())
reg_all_params = reg_all_params %>%
    mutate(rank = row_number())


ggplot(ran_all_params, aes(x=rank, y=loglik)) +
  geom_point(size=.1)
ggplot(reg_all_params, aes(x=rank, y=loglik)) +
  geom_point(size=.1)


          # loglik         q p sd_approx avg_comp sd_comp rank


params %>% group_by(cond) %>%
            summarise(mean = ci(param)[1], 
                      lowCI = ci(param)[2],
                      hiCI = ci(param)[3], 
                      sd = ci(param)[4])
```

