{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Sketch Partitions for Rating Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the sketch paths from that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = os.path.abspath('../..')\n",
    "exp_name = 'classify_iternum'\n",
    "exp_dir = os.path.join(proj_dir,exp_name)\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "\n",
    "full_stim_paths = os.listdir(sketch_dir) # list out all the sketches in that directory\n",
    "sketches = [i for i in full_stim_paths if i.split('/')[-1] != '.DS_Store']\n",
    "\n",
    "stimListDir = os.path.abspath('../../experiments/classify_iternum/stimList')\n",
    "\n",
    "if not os.path.exists(stimListDir):\n",
    "    os.makedirs(stimListDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble a dataframe from all the sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_info = pd.DataFrame(columns = [\"orig_gameid\",\"orig_animal\",\"orig_cardinality\",\"orig_trial\",\"orig_cond\",\"orig_version\",\"sketch_url\"]) # initialize dataframe\n",
    "\n",
    "for i in range(len(sketches)): # for every sketch\n",
    "    name = sketches[i].split('_') # split up its metadata\n",
    "                                                        #    gameID         animal            cardinality               trialnum       condition      stim_version\n",
    "    stimurl = \"https://iternum-sketches.s3.amazonaws.com/\" + name[0] + '_' + name[1] + '_' + str(int(name[2])+1) + '_' + name[3] + '_' + name[4] + '_' + name[5]\n",
    "\n",
    "    # following two lines are dead:\n",
    "#     stimID = name[4].split('_') # ... by multiple delimiters\n",
    "#     stimurl = \"https://iternum-sketches.s3.amazonaws.com/\" + name[0] + '_' + stimID[0] + '_' + str(int(stimID[1])-1) + '_' + name[2] + '_' + name[3] + '_' + name[4]     \n",
    "    row = np.array([name[0],name[1],str(int(name[2])+1),name[3],name[4],name[5].split('.')[0],stimurl]) # put into relevant column\n",
    "    sketch_info.loc[len(sketch_info)] = row # now append that to the sketch info dataframe\n",
    "    \n",
    "sketch_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_info.iloc[0]['sketch_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = sketch_info    # the bag of sketches to sample, because sampling without replacement\n",
    "\n",
    "games = bag.orig_gameid.unique() # we want this to be a unique list of all the games\n",
    "paradigms = [] # this will be a list of dataframes, each dataframe containing the sketches to be rated by a rater\n",
    "\n",
    "\n",
    "batch = 0\n",
    "while len(bag) > 0: # sample from the bag without replacement\n",
    "    \n",
    "    # initialize paradigm \n",
    "    number_paradigm = {'versionID':batch,        # which partition is it (set of sketches) ?\n",
    "                       'classify_condition':'number',  # what feature will mturk recognizers be asked to classify?\n",
    "                       'games':[],               # empty list to be filled with classification games as they happen    \n",
    "                       'meta':[]}                # the whole [unordered] trial list goes in this 'meta' structure\n",
    "    shape_paradigm = {'versionID':batch+1,        # which partition is it (set of sketches) ?\n",
    "                       'classify_condition':'shape',  # what feature will mturk recognizers be asked to classify?\n",
    "                       'games':[],               # empty list to be filled with classification games as they happen    \n",
    "                       'meta':[]}                # the whole [unordered] trial list goes in this 'meta' structure\n",
    "\n",
    "    \n",
    "    for i in range(len(games)): # we want each rater to see [no more than] one sketch from each game\n",
    "        trial = {} # initialize a dictionary for this rater, 1 game per trial\n",
    "        \n",
    "        row = bag[bag['orig_gameid']==games[i]].sample(n=1,replace=False,random_state=333) # sample a sketch at random from the game\n",
    "        bag = bag.drop(index = row.index) # remove it from the bag\n",
    "        \n",
    "        trial[\"orig_gameid\"] = row.iloc[0][\"orig_gameid\"]\n",
    "        trial[\"orig_animal\"] = row.iloc[0][\"orig_animal\"]\n",
    "        trial[\"orig_cardinality\"] = row.iloc[0][\"orig_cardinality\"]\n",
    "        trial[\"orig_trial\"] = row.iloc[0][\"orig_trial\"]\n",
    "        trial[\"orig_cond\"] = row.iloc[0][\"orig_cond\"]\n",
    "        trial[\"orig_version\"] = row.iloc[0][\"orig_version\"]\n",
    "        trial[\"sketch_url\"] = row.iloc[0][\"sketch_url\"]\n",
    "        \n",
    "        number_paradigm['meta'].append(trial)\n",
    "        shape_paradigm['meta'].append(trial)\n",
    "    \n",
    "    # when a paradigm is assembled, put it into the list:\n",
    "    paradigms.append(number_paradigm) # first put the version for people classifying the shape info     \n",
    "    paradigms.append(shape_paradigm) # then store the exact same data structure but change the classification goal\n",
    "    \n",
    "    batch += 2\n",
    "    \n",
    "num_partitions = len(paradigms)    \n",
    "print('We have {} unique partitions.'.format(num_partitions)) # Should be 32*2=64 paradigms of 61 sketches; each rater sees one per game, requiring 64 raters        \n",
    "\n",
    "# print(paradigms[1].iloc[3,6])    # print one of the urls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in paradigms:\n",
    "#     print(i['versionID'], i['classify_condition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put that datastructure into Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "import pymongo as pm\n",
    "import socket\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1:27017') \n",
    "db = conn['stimuli']\n",
    "coll = db['iternum_classification']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now really insert data\n",
    "reallyRun = False\n",
    "if reallyRun:\n",
    "    for (i,j) in enumerate(paradigms):\n",
    "        print ('%d of %d uploaded ...' % (i+1,len(paradigms)))\n",
    "        clear_output(wait=True)\n",
    "        coll.insert_one(j)\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coll.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No longer relevant: Convert dictionary of dictionaries to an array of JSON objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_paradigm = paradigms[0]\n",
    "# this_paradigm.iloc[0]['URL']\n",
    "# this_paradigm.to_json(orient='records')\n",
    "\n",
    "## Objective: to save out a stimList.js that contain a dictionary of dictionaries\n",
    "## stimList = { {'versionID': 0, 'meta':{...}} , }\n",
    "## \"versionID\" refers to the specific partition ID\n",
    "## \"meta\" refers to the metadata corresponding to that partition, e.g., paradigms[0]\n",
    "\n",
    "# filename = \"stimList.js\"\n",
    "# pathname = os.path.join(stimListDir,filename)\n",
    "# with open(pathname, 'w') as the_file:\n",
    "#     the_file.write(str(paradigms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following block of code is no longer relevant:\n",
    "\n",
    "# for i in range(len(paradigms)):\n",
    "#     paradigm = paradigms[i]\n",
    "#     paradigm.to_csv(batchdir + '/batch_{}.csv'.format(str(i+1)))\n",
    "    \n",
    "    #had it saving to jsons earlier but I don't think it worked wells\n",
    "#     js = paradigm.to_json()\n",
    "#     with open(batchdir + '/batch_{}.csv'.format(str(i+1)), 'w') as outfile:\n",
    "#         json.dump(js, outfile)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
